RNN은 기본적으로 '자기회귀'의 성질을 가지고 있다.
처음 한 번의 연산이 되고 다시 돌아가서 또 한 번의 연산이 이루어진다.

# 1. simpleRNN parameter 계산법

> num_units^2 + num_units*input_dim + num_units = (n^2 + nm + n )
> (num_units + input_dim + 1) * num_units

simpleRNN의 unit 갯수가 10개, input_dim = 1이면
(10 + 1 + 1)*10 = 120 으로 parameter는 120이 나온다.

# 2. LSTM(Long Short Term Memory) parameter 계산법

> (num_units * num_units + num_units * input_dim  + num_units) * 4 = (n^2 + nm + n ) * 4  
> (num_units + input_dim + 1) * num_units * 4  

LSTM의 unit 갯수가 10, input_dim = 1이면
(10 + 1 + 1)*10*4 = 480 으로 parameter는 480이 나온다.

# 3. GRU(Gated Recurrent Unit) parameter 계산법
> (num_units * num_units + num_units * input_dim + num_units) * 3 = (n^2 + nm + n ) * 3
> (num_units + input_dim + 1) * num_units * 3

GRU의 unit 갯수가 10, input_dim = 1이면
(10 + 1 + 1) * 10 * 3 = 360 으로 parameter는 360이 나온다.

4. simpleRNN, LSTM, GRU의 비교

simpleRNN은 가장 기본적이고 단순한 모델이다. 
셋 중 연산의 수가 가장 작다.

LSTM은 가장 많이 쓰이는 모델로, 장기의존성 문제를 해결하기 위해 만들어졌다.
오랜 기간동안 정보를 기억하는 것이 LSTM의 특징!
LSTM은 simpleRNN과는 parameter의 수가 확연히 차이가 나고 성능이 훨씬 좋다.
LSTM은 내부적으로 4번의 연산이 이루어지기 때문에 parameter 계산식에서 4를 곱해준다.

GRU는 LSTM과 구조상의 큰 차이는 없다.  
LSTM을 보완하기 위해 만든 것인데 약간의 변경(혹은 축소)을 했다고 보면 된다.
분석결과에도 큰 차이가 없는 것으로 알려져있는데 이 말은 성능상의 큰 차이가 없다는 것이 아니라
주제별로 LSTM이 좋기도 GRU가 좋기도 하다는 의미!
LSTM에 비해 학습할 가중치가 적다. 그래서 LSTM보다 속도가 약간 빠르다.
GRU는 내부적으로 3번의 연산이 이루어져서 parameter 계산식에서 3을 곱한다.