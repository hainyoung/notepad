0630_day37

https://wdprogrammer.tistory.com/33
<Regularization과 딥러닝의 일반적인 흐름 정리>

최적화(Optimization) : 가능한 훈련 데이터에서 최고의 성능을 얻으려고 모델을 조정하는 과정
일반화(Generalization) : 훈련된 모델이 이전에 본 적 없는 데이터에서 얼마나 잘 수행될 수 있는지를 의미
- 하지만, 일반화 성능을 제어할 방법이 없기 때문에 단지 훈련 데이터를 기반으로 모델을 조정할 수 있음

- 과소적합(Underfitting) : 훈련 데이터의 손실이 낮아질수록 테스트 데이터의 손실도 낮아진다
- 즉, 모델 성능이 계속 발전될 여지가 있다

- 과대적합(Overfitting) : 모델이 훈련 데이터에 특화되어 일반화 성능이 높아지지 않고 오히려 감소
- 즉, 훈련 데이터에 특화된 패턴을 학습하기 시작했다는 의미
- 이 패턴은 새로운 데이터와의 관련성이 적어 잘못된 판단을 하게 만든다


# Regularization : 과대적합, overfitting을 피하는 처리 과정
1) 가장 좋은 방법은 더 많은 훈련 데이터를 모으는 것
2) 차선책은 모델이 수용할 수 있는 정보의 양을 조절하거나 저장할 수 있는 정보에 제약(constraint)을 가하는 것
3) 모델에 있는 학습 파라미터의 수를 줄이는 것(학습 가중치)
- 파라미터의 수는 층의 수와 각 층의 유닛 수에 의해 결정된다(layer, node)
- Capacity of model : 학습 파라미터 수
4) 적절한 layer 수와 capacity 크기를 결정해야 함, 작게 시작해서 검증 손실에 따라 크게 하면서 모델 구축
5) weight regularization(가중치 규제) : 네트워크의 복잡도에 제한을 두어 가중치가 작은 값을 가지도록 강제하는 것
- 이렇게 하면 가중치 값의 분포가 더 균일하게 된다
(1) L1 regularization : 가중치의 절댓값에 비례하는 비용이 추가(가중치의 L1 norm)
(2) L2 regularization(=weight decay) : 가중치의 제곱에 비례하는 비용이 추가(가중치의 L2 norm)
6) Dropout : 입력 벡터 중 일부를 무작위로 0으로 바꿈, 테스트 단계에서는 어떤 유닛도 dropout 되지 않으나
대신 층의 출력을 drop out 비율에 비례해서 줄여준다
Dropout 비율 : 0이 될 특성의 비율, 비율이 높을수록 0이 많이 됨, 보통 0.2 ~ 0.5 사이

* Occam's razor(오캄의 면도날) 이론 : 어떤 것에 대한 두 가지 설명이 있다면 더 적은 가정이 필요한 간단한 설명이 옳을 것이라는 이론


1) 문제 정의와 데이터셋 수집
- 입력 데이터는 무엇인가
- 어떤 것을 예측하려 하는가
- 가용한 훈련 데이터가 있어야 어떤 것을 예측하도록 학습할 수 있다
- 당면한 문제는 어떤 종류인가?
(이진 분류, 다중 분류, 스칼라 회귀, 벡터 회귀, 다중레이블 다중 분류, 군집, 생성, 강화학습 등)
- 문제의 유형을 식별하면 모델의 구조와 손실 함수 등을 선택하는 데 도움이 된다
- nonstationary problem(시간에 따라 변하는 문제) 
: 최근 데이터로 주기적으로 훈련하거나 , 시간 분포에 맞게 데이터를 수집하여 시간에 따라 변하지 않는 문제로 바꾼다

# 입력과 출력이 무엇인지와 어떤 데이터를 사용할 것인지 알아야 함
(1) 주어진 입력으로 출력을 예측할 수 있다고 가설을 세운다
(2) 가용한 데이터와 입력과 출력 사이의 관계를 학습하는 데 충분한 정보가 있다고 가설을 세운다

2) 성공 지표 선택
- 클래스 분포가 균일한 분류 문제에는 정확도, roc, auc가 일반적인 지표
- 클래스 분포가 균일하지 않은 문제에서는 정밀도, 재현율 사용
- 랭킹 문제나 다중 레이블 문제에는 평균 정밀도를 사용할 수 있음

3) 평가 방법 선택
- Hold-out validation set 분리 : 데이터가 풍부할 때 사용(그냥 검증 데이터만 나누는 것)
- K-fold cross validation : 홀드아웃 검증을 사용하기에는 샘플의 수가 너무 적을 때 사용
- 반복 k-fold cross validation : 데이터가 적고 매우 정확한 모델 평가가 필요할 때 사용

4) 데이터 준비
- feature마다 범위가 다르면 정규화해야 한다
- feature engineering을 수행할 수 있다(특히 데이터가 적을 때)

5) 기본보다 나은 모델 훈련하기
- 마지막 층의 활성화 함수
- 손실 함수 : 성공 지표에 대한 손실 함수로 바꿀 수 없는 경우도 있음(roc, auc)
- 최적화 함수 설정

문제 유형				마지막 층 activation		loss func
binary classification			sigmoid				binary_crossentropy
단일 label multi classification		softmax				categorical_crossentropy
다중 label multi classfication		sigmoid				binary_crossentropy
임의 값에 대한 regression		없음				mse(mean squared error)
0과 1 사이 값에 대한 regression	sigmoid				mse or binary_crossentropy


6) 과대 적합 모델 구축
- 충분한 층과 파라미터가 있는지 확인
- 층을 추가
- 층의 크기를 키운다(학습 파라미터 수 증가)
- 더 많은 epoch 동안 훈련
- 모델 성능이 감소하기 시작했을 때 과대적합에 도달한 것
- 규제와 모델 튜닝을 통해 이상적인 모델에 가깝게 만든다



7) 모델 규제와 하이퍼파라미터 튜닝
- dropout 추가
- 층을 추가하거나 제거해서 다른 구조를 시도
- L1이나 L2 또는 두 가지 모두 추가
- 하이퍼파라미터 조정(유닛 수, 학습률)
- 선택적으로 특성 공학을 시도해본다(feature engineering)
- 검증 데이터에 과대적합되지 않도록 조심



http://hleecaster.com/ml-overfitting/
< 머신러닝 오버피팅의 개념과 해결 방법>
- 머신러닝은 쉽게 이야기하자면, 
- 대량의 데이터를 알고리즘에 넣어서 일종의 규칙을 생성하고, 그 규칙에 따라 입력값을 분류하도록 하는 것이다. 
- 그래서 이 알고리즘에 제공하는 학습 데이터가 매우 중요하다
- 학습 데이터의 모든 값들을 하나하나 살펴보면서 규칙을 생성하기 때문이다

# Overfitting(과적합)이란
- overfitting은 모델의 파라미터들을 학습 데이터에 너무 가깝게 맞췄을 때 발생하는 현상
- 학습 데이터가 실제 세계에서 나타나는 방식과 완전히 똑같을 거라고 가정해버리는 것
- 그래서 학습 데이터 세트에 속한 각각의 개별 데이터들을 완벽하게 설명하기 위한 모델을 생성한다
- 이렇게 말하면 좋아 보일 수도 있는데 현실은?


http://hleecaster.com/ml-overfitting/

- 위 블로그의 그래프 참조
- 각 점들은 학습 데이터, 선은 머신러닝이 생성한 모델이라고 볼 수 있다
- 이 모델은 개별 점들을 하나도 놓치고 싶지 않아서 이 현상을 어떻게든 완벽하게 설명하고 싶어 요리조리 따라가며 ㅅ너을 그림
- 이 현상이 바로 overfitting
- 이 모델(선)은 과적합이 일어난 모델이다
- 실제 현상도 이렇게 구불구불한 선을 따라서 나타날까?
- Nope
- 큰 그림을 볼 필요가 있다 
- 그래야 전체적인 추세나 패턴이 보이고 제대로 된 insight를 얻어서 예측을 해낼 수 있기 때문
- 만약 학습 데이터를 통해 전체적인 패턴을 유추하면 구불구불하지 않은 매끄러운 하나의 곡선을 그릴 수 있다
- 이 선, 즉 모델이 좋은 모델이라고 할 수 있다
- 결국, overfitting은 너무 세밀하게 학습 데이터 하나하나를 다 설명하려고 하니 정작 중요한 패턴을 설명할 수 없게 되는 현상이다
- 학습 데이터에 지나치게 의존하면 거기에 포함 된 다양한 편견이나 불공정한 내용까지 모두 모델이 답습하게 된다
- "Garbage In , Garbage Out"

# Overfitting 문제를 해결하는 방법
1. 일단 데이터부터 자세히 살펴보기
- 데이터에서 중요한 통계치들을 면밀히 봐야 한다
- 변수들의 평균과 중앙값은 물론, 예를 들면 pandas의 groupby 와 같은 메서드를 사용해서 반드시 집단별 통계치를 확인해야 한다
- 그게 곧 머신러닝이 학습하게 될 패턴이므로
- 데이터를 확인할 때 가장 도움되는 것은 시각화, visualization이다
- 시각화만 잘 해도 중요한 패턴이 우리 눈에 띈다

2. 애초에 적절하게 수집된 데이터인지 확인
- 머신러닝 모델을 적용하고자 하는 모든 집단으로부터 골고루 수집된 데이터인지 확인할 필요가 있다
- 수집된 데이터가 특정 장면에만 적용되는 것이라면 애초에 보편적으로 적용할 수 있는 모델이 아닌 것
- 특정 집단의 특성만 반영할 가능성이 높다
- 데이터 수집 단계에서부터 신중했는지 검토해야 한다

3. 학습 데이터 보강, Augmentation 하기
- 예를 들어 불법 신용카드 거래를 감지하는 머신러닝 모델을 생성한다고 가정
- 우선 모든 거래 중 불법 거래는 거의 없을 테니 모든 거래가 합법적이라고 에측하는 무식한 모델을 만들어도 매우 높은 정확도를 얻을 수 있음
- 그 높은 정확도가 신뢰성이 있는가? Nope
- 이 문제를 해결하려면 학습 데이터에 내가 가지고 있던 사례의 특성들을 조금씩 조작해서 추가시켜주는 방법이 있다
- 위 예시에서 만약 애초에 내가 가지고 있던 학습 데이터에 사기 거래가 2건밖에 없었다면
- 이 알고리즘은 그 2건이 가진 특성에만 완전 꽂혀버려셔 overfitting이 발생할  수밖에 없을 것
- 그래서 조금씩 다른 사기 거래 케이스를 조작적으로 추가시켜서 다양한 케이스를 확보하여 overfitting을 줄여주는 게 필요
- 이를 Data Augmentation 이라 부른다
- 이는 이미지 분류에서 특히 많이 사용된다
- 동물 사진을 분류하는 모델이라고 하면, 멀쩡한 이미지뿐만 아니라 그 사진을 회전시키거나 찌끄러지게 만드는 방식으로
- 여러 수정본들을 함께 학습시킨다

4. 학습 데이터에 포함될 특성(featurest)을 제한
- 어떤 특성이 모델에 영향을 과도하게 미치는 경우 오히려 해당 특성을 제거하고 모델을 학습하는 전략을 짜볼 수 있다
- 안 그러면 다른 모든 특성이 다 똑같더라도 특정한 특성(예를 들어, 성별 , 나이 등) 하나가 다르다는 이유만으로
- 분류나 예측을 수행하게 되는 건데 이건 결국 모델이 현실에서 나타나는 편견이나 차별을 그대로 반영한다는 뜻이기 때문
- Garbage In, Garbage Out
- 이상적으로 생각하면 모델이 어떤 특성을 바탕으로 분류/예측을 하길 기대하는지 잘 생각해보고
- 정말 필요하다고 생각하는 특성들을 포함시켜야 한다

5. 결론
- 머신러닝 알고리즘은 학습 데이터를 들여다보면서 가정과 규칙을 세우기 때문에 
- 어차피 어떤 식으로든 일종의 편향을 만들어낼 것이다
- 우리는 학습 데이터에 편향이 내재되어 있다는 것을 인정하고 시작하는 게 좋다
- 그리고 만약 편향을 어떤 식으로 보상하거나 그 효과를 상쇄시키려고 개입하는 행위는
- 결국 데이터에 또 다른 편향을 심게 되는 꼴이기 때문에 주의해야 한다
- 결론은 균형을 잡는 것이 중요하다는 말
- 현실에 이미 편향이나 차별, 기울어진 운동장이 존재한다는 것을 공개적으로 언급하고
- 모든 이해관계자들이 이 사실을 인정한 상태에서 모델을 활용해야 한다는 것





























