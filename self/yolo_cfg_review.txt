yolo.cfg review

참조 
# darknet 훈련예시
https://eehoeskrap.tistory.com/370

# CFG Parameters in the [net] section
https://github.com/AlexeyAB/darknet/wiki/CFG-Parameters-in-the-%5Bnet%5D-section

# stpes에 관한 상세 설명
https://stackoverflow.com/questions/50390836/understanding-darknets-yolo-cfg-config-files

# momentum 설명
https://light-tree.tistory.com/140

[net]
# Testing
# batch=1
# subdivisions=1

# Training
batch=32
- 전체 데이터에서 몇 개씩 작업할 것인가
- if, 320개의 데이터를 train 한다면, 32개씩 잘라주게 됨. 
- 여기서 하나의 batch size가 train 되면 1iteration.
- 1 epoch는 전체 데이터가 훈련된 것이기 때문에 10 iteration == 1 epoch 

subdivisions=32
- subdivsions 의 사전적 의미 : 재분(다시 나눈다)
- subdivisons의 개념과 함께 minibatch의 개념을 알아야 한다
- darknet github에 따르면 
subdivisions : number of mini_batches in one batch, size mini_batch = batch/subdivisions, 
so GPU processes mini_batch samples at once, 
and the weights will be updated for batch samples (1 iteration processes batch images)
- 즉 GPU는 한 번에 mini batch sample 수만큼 작업, 처리한다
- min batch == batch / subdivisions
- 수치로 예를 들자면, batch == 32, subdivisions == 32 로 설정했다면
- mini batch == 1이 된다
- GPU는 데이터 하나씩 작업한다(?)(현재 우리 이미지 데이터로 따지면 1장씩?)
- 한 번의 배치사이즈를 훈련할 때 (batch == 32) 32개씩 잘라서 작업하는데 그 안에서 또 하나씩만 처리한다?
- train 할 때 batch 64, sub 16으로 주니 out of memory 발생해서 32를 주니까 돌아가다가 또 out of memory
- darknet 훈련 예시 블로그에 따르면,,,
- subdivisions 는 배치 사이즈인 64를 얼마나 쪼개서 학습을 할건지에 대한 설정 값
- 기본 값은 8 이지만 Out of memory 에러가 날 경우 16 또는 32 또는 64 로 조절하여 학습을 시도함
- 즉, subdivisions이 작을수록 mini batch 크기가 커지는 만큼 GPU에 부담도 커진다?
- mini batch에 대한 공부가 더 필요


출처: https://eehoeskrap.tistory.com/370 [Enough is not enough]


width=416
- network size (width), so every image will be resized to the network size during Training and Detection

height=416
- network size (height), so every image will be resized to the network size during Training and Detection

- width, height 를 cfg 파일에서 설정할 수 있다
- 이미지 크기 결정
- 훈련 전에 리사이즈를 할 수 있는 방법

channels=3
- network size (channels), so every image will be converted to this number of channels during Training and Detection
- 채널 값
- 1로 한다면 흑백 이미지로 훈련 시킬 수 있을 것 같다

momentum=0.9
- accumulation of movement, how much the history affects the further change of weights (optimizer)
- 쉽게 생각하면 과적합 방지?
- 훈련 될 때마다 생성되는 새로운 가중치에 너무 치중 되지 않도록
- 이전에 생성된 가중치들의 평균을 반영(쉽게 말해서!)해서 새로운 가중치를 너무 신뢰하지 않게끔 설정해두는 것


decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.001
burn_in=1000
max_batches = 6200
policy=steps
steps=4800, 5400
scales=.1,.1

[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky